\documentclass[12pt]{article}

% --- Core math & layout
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

% --- Figures & tables
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{array}        % for \newcolumntype
\usepackage{makecell}
\usepackage{float}        % [H] placement
\usepackage{adjustbox}    % fit tables to width
\usepackage{pdflscape}    % landscape pages (rotates in PDF viewer)
\usepackage{caption}

% --- Text handling & lists
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{enumitem}

% --- URLs & references
\usepackage[hyphens]{url}
\usepackage{natbib}
\usepackage{hyperref}     
\hypersetup{
  colorlinks=true,
  linkcolor=black,
  citecolor=black,
  urlcolor=blue
}

% --- House style tweaks
\setlength{\emergencystretch}{1.8em}
\hyphenpenalty=500 \exhyphenpenalty=500

% TabularX helper: ragged-right X col
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}

% --- Optional headers/footers (keep if you want them)
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\rhead{HRL Formalization}
\lhead{Daniel Bourdon}
\rfoot{\thepage}


\begin{document}

\title{Research Topics: Hierarchical Reinforcement Learning for Conversational Museum Agents: Structured Engagement via Gaze and Language Models}
\author{Daniel Bourdon \\ University of Twente}
\date{\today}
\maketitle\section{Introduction}
Museums are still working to rebuild steady, frequent visitation after the pandemic, and the fight for attention—on-site and online—has only grown. Recent surveys show the share of U.S. adults who visited a museum in the past year is back to roughly pre-pandemic levels (about 28\%), yet overall recovery remains uneven, largely because repeat visits have dipped rather than interest itself \citep{aam2023survey,aam2023gaps}. In the UK, national figures likewise report a sharp rise in both physical and digital engagement through 2023/24, pointing to strong demand for richer, more tailored experiences \citep{dcms2024engagement}. The practical challenge follows: deliver interactions that flex to a visitor’s goals, time, and prior knowledge—without scaling staff one-to-one.\\

Conversational agents (CAs) become genuinely useful once they move past scripted menus and sustain real dialogue. Early deployments in museums—such as the embodied guide Max \citep{kopp2005conversational} and the twin agents Ada and Grace \citep{traum2012ada}—showed how small talk and naturalistic guidance can enliven galleries. But many first-generation systems leaned on rules and brittle flows: they were expensive to update and struggled when visitors wandered off script or wanted deeper context. Cross-domain reviews echo these limits for rule-based and narrowly scoped chatbots: they’re sensitive to out-of-distribution queries, prone to escalation under ambiguity, and incur rising maintenance costs as trees grow \citep{laymouna2024jmir,gruenhagen2024education}. In short, museums need agents that can reason over long, multi-turn exchanges while staying grounded in curatorial knowledge and audience needs.\\

Reinforcement learning (RL) is a natural fit for sequential decision-making of this kind. An RL agent learns through interaction to maximize cumulative reward \citep{sutton2018reinforcement}, which maps well onto selecting dialogue policies under uncertainty. Still, a single flat policy can falter in open-ended conversation: it must juggle near-term coherence with longer-horizon goals like learning outcomes and sustained engagement. Hierarchical reinforcement learning (HRL) addresses this by breaking behavior into temporally extended sub-policies (options), supporting decisions at multiple levels of abstraction and improving credit assignment across long spans \citep{sutton1999between,dietterich2000maxq}. In dialogue terms, that means high-level choices (e.g., \emph{explain}, \emph{ask}, \emph{transition}) with low-level realizations that adapt in the moment—an approach backed by recent work on hierarchical policies in both task-oriented and open-domain settings \citep{saleh2020hierarchical,wang2021hdno}.\\

In museums, agents must do two things at once: deliver accurate, situationally relevant content and keep people engaged. Measuring engagement in a robust, ethical way on the floor is non-trivial. Multimodal work suggests that simple, observable gaze signals—such as dwell time on an object of interest—track attention and cognitive effort in interactive learning contexts, including VR and situated experiences \citep{adhanom2023vrreview,moreno2024vrsurvey}. These measures are appealing in practice: they can be captured passively (e.g., via VR/AR or instrumented exhibits) and distilled into compact features that correlate with on-task behavior.\\

Against this backdrop, the thesis examines HRL for museum dialogue with the goal of structuring policy decisions across short, medium, and long conversational horizons. Engagement and learning support are treated as co-equal objectives, with gaze-derived attention features serving as a principled, low-intrusion proxy for engagement. We outline the intended direction as: (i) define high-level dialogue options aligned with curatorial aims; (ii) ground actions in institution-specific knowledge; and (iii) test whether hierarchical policies improve persistence, topic coverage, and visitor-reported usefulness relative to rule-based baselines and non-hierarchical policies. Earlier embodied and virtual guides \citep{kopp2005conversational,traum2012ada} motivate the setting; recent progress on adaptive agents supports real-time responsiveness to user state \citep{woo2024adaptive}.\\

In sum, we recast museum conversational guidance as a hierarchical control problem with measurable engagement signals—linking longstanding interpretive goals to modern policy learning while staying mindful of museum operations.

\section{Background and Related Works}

Museums are dynamic environments where visitors engage with exhibits in diverse and unpredictable ways. Conversational agents deployed in such settings must navigate a range of challenges to provide meaningful and engaging interactions.

\subsubsection*{Diverse Visitor Profiles, Engagement Balance, and Context Awareness}

Museum visitors vary widely in background, prior knowledge, motivations, and interests; as a result, conversational agents must personalize both content and pacing to sustain relevance and learning. Decades of museum-learning research highlight that visitors’ experiences are shaped by personal, sociocultural, and physical contexts, reinforcing the need for adaptive dialogue rather than one-size-fits-all scripts \citep{falk-dierking-2016-museum}. Recent sector-wide surveys similarly show heterogeneous expectations around “community connection” and interaction styles across audience segments, underlining the value of personalization in digital mediation \citep{aam-2024-data-story,aam-2024-staff}. In practice, museum chatbots too often under-measure visitor experience—revealing a gap between deployment and rigorous evaluation of how different audiences engage—so systems should incorporate continuous UX assessment and user modeling \citep{stekerova-2022-chatbots}.\\

Balancing educational depth with enjoyable, conversational flow remains a core challenge. Empirical deployments in museums and cultural spaces show that thoughtfully designed conversational/interactive systems can lift engagement and satisfaction when they adapt the level of detail and the interaction style to the visitor (e.g., task-oriented vs.\ exploratory) \citep{robinson-etal-2008-ask,chai-arayalert-2024-chatbot,chin-2024-wearableMR}. Emerging work on human-centric AI in museums echoes this, advocating visitor-centered adaptivity and participatory design to avoid over-standardized narratives and to better serve both casual and expert visitors \citep{derda-2025-human-centric}.\\

Doing this well requires strong context awareness—tracking where the visitor is, what they have already seen/asked, and immediate behavioral cues (e.g., dwell, movement), then using that state to time interventions and choose the right granularity. The museum-tech literature (from mobile guides to AR) shows that location and behavior signals enable timely, relevant guidance, while contemporary HCI surveys argue for richer, multimodal context models that combine spatial, historical, and interaction data for adaptive interfaces \citep{raptis-2005-context,wang-2022-museum-AR-survey,hu-2025-context-aware}. Together, these findings establish three intertwined design problems for museum conversational agents: (1) personalization for diverse visitor profiles, (2) a principled balance between education and enjoyment, and (3) robust context awareness to deliver the right content at the right moment.

\subsection{Limitations of Traditional Dialogue Systems}

Building on the need to serve diverse visitor profiles while balancing engagement with learning and responding to context, early museum conversational agents (CAs) demonstrated the value of social dialogue, small talk, and timely guidance but were predominantly rule-based and brittle under visitor variability. Systems like the embodied guide \emph{Max} \citep{kopp2005conversational} and the virtual twins \emph{Ada} and \emph{Grace} \citep{traum2012ada} showed that naturalistic language and social behaviors can enrich the gallery experience. Cross-domain reviews echo the limitations of such scripted or menu-driven systems: sensitivity to out-of-distribution queries, escalating fallback behavior under ambiguity, compounding maintenance as decision trees expand, and difficulty incorporating continuous evaluation of visitor experience \citep{laymouna2024jmir,reimann2024dm}. In dialogue-system research more broadly, policy control based on fixed rules is known to degrade with noise and ambiguity, motivating probabilistic decision-making and learned policies \citep{williams2007pomdp,young2013pomdp}. These observations motivate moving beyond static flows to policies that adapt over multi-turn interactions, remain grounded in curatorial content, and can trade off engagement with coverage of educational goals.\\

As mentioned just above, dialogue systems that rely on predefined scripts or rule-based frameworks limit their flexibility and adaptability in dynamic environments like museums. These systems struggle with:
\begin{itemize}
\item \textbf{Scalability}: Difficulty in scaling to the large combinatorial space of visitor intents, exhibit contexts, and follow-up questions; maintaining trees/menu flows becomes costly and brittle \citep{reimann2024dm}.
\item \textbf{Personalization}: Limited capacity to adjust content depth and style to individuals or segments, compared with learned policies that condition actions on user state/history \citep{kwan2023dplsurvey}.
\item \textbf{Contextual Integration}: Challenges in fusing real-time signals (location, dwell/engagement, prior turns) into action selection; robust context use typically requires probabilistic state tracking and policy optimization \citep{williams2007pomdp,reimann2024dm}.
\end{itemize}\\

These limitations highlight the need for more adaptive and intelligent dialogue management approaches capable of operating effectively in complex, variable museum settings. In practice, architectures for social CAs often grow into multi-component pipelines (speech/vision, user and domain models, affect, timing/heuristics) as shown in Figure~\ref{fig:complex-architecture}, where layered emotion models and cognitive components coordinate action selection \citep{opdenakker2012computational}. While such architectures capture necessary facets of social interaction, their breadth underscores a core challenge: hand-authoring rules and heuristics at each layer does not scale to the variability and uncertainty of live museum floors.\\

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{Images/Introduction/complex.png}
\caption{Conceptual architecture for social CAs: hand-crafted rules/heuristics govern dialogue and reasoning; perception uses recognition modules (speech, vision, emotion) with context and affect layers feeding action selection. Adapted from \citet{opdenakker2012computational}.}
\label{fig:complex-architecture}
\end{figure}

The complexity depicted in this architecture emphasizes the challenges faced when attempting to model human-like conversational behaviors. Consequently, exploring alternative approaches grounded in \emph{learning} rather than scripting becomes natural. In dialogue research, Reinforcement Learning (RL) has been used to optimize dialogue policies under uncertainty—framing the user as the environment and the system as the agent—improving robustness to ambiguity and enabling principled trade-offs (e.g., engagement vs.\ coverage) \citep{williams2007pomdp,kwan2023dplsurvey}. Moreover, \emph{hierarchical} RL (HRL) decomposes policy decisions across levels (high-level options vs.\ low-level actions), addressing large action/state spaces and enabling adaptivity over longer horizons; feudal/option-based managers have shown scalability gains in multi-domain dialogues \citep{casanueva2018feudal,weisz2018sample,su2017sample}. This motivates our turn to RL/HRL for museum CAs in the following parts.\\

\subsection{Markov Decision Processes}

Reinforcement Learning (RL) provides a framework for agents to learn optimal behaviors through interactions with an environment. This interaction is commonly modeled as a Markov Decision Process (MDP), defined by the tuple $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$, where:

\begin{itemize}
    \item $\mathcal{S}$: a finite set of states,
    \item $\mathcal{A}$: a finite set of actions,
    \item $P(s'|s,a)$: the probability of transitioning to state $s'$ from state $s$ after taking action $a$,
    \item $R(s,a)$: the immediate reward received after taking action $a$ in state $s$,
    \item $\gamma \in [0,1]$: the discount factor that prioritizes immediate rewards over distant future rewards.
\end{itemize}

To better illustrate the agent-environment interaction underlying Markov Decision Processes, Figure~\ref{fig:mdp-loop} provides a high-level schematic adapted from \citet{sutton2018reinforcement}. It highlights how the agent observes the current state $s_t$, selects an action $a_t$, and receives both a new state $s_{t+1}$ and a scalar reward $R(s_t, a_t)$ from the environment in response.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/Introduction/BASIC.png}
    \caption{Basic agent-environment interaction loop in reinforcement learning. Adapted from \citet{sutton2018reinforcement}.}
    \label{fig:mdp-loop}
\end{figure}

The agent's objective is to learn a policy $\pi(a|s)$ that maximizes the expected cumulative reward:

\[
J(\pi) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \right]
\]

\subsection{Value-Based and Policy-Based Methods}

RL algorithms are broadly categorized into:

\begin{itemize}
    \item \textbf{Value-Based Methods}: These focus on estimating the value function $V(s)$ or the action-value function $Q(s,a)$, which represent the expected return from a state or state-action pair, respectively. An example is Q-learning, where the update rule is:

    \[
    Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \right]
    \]

    \item \textbf{Policy-Based Methods}: These directly optimize the policy $\pi(a|s)$ without relying on value functions. Techniques like policy gradients adjust the policy parameters in the direction that increases expected rewards.

\end{itemize}

Despite its suitability, applying RL to dialogue systems presents several challenges:\\

Several challenges consistently arise when using reinforcement learning for dialogue systems. First, there is the \textbf{credit assignment problem}: it is often unclear which parts of a conversation actually contributed to a good outcome, especially when feedback is sparse or only available at the end of the interaction. This ties closely to the issue of \textbf{sparse and delayed rewards}, where meaningful signals like user satisfaction may only be known after a full dialogue has ended, making it difficult for the system to learn which actions were effective \citep{peng2017composite}. Another practical challenge is \textbf{data efficiency}. Reinforcement learning typically requires many interactions to learn good policies, which can be impractical to collect in real-world conversational settings. Finally, as the complexity of conversations grows, the \textbf{state and action spaces expand rapidly}, making it hard for an agent to learn effective policies without techniques like abstraction or hierarchical decomposition to keep learning tractable.\\


These challenges are particularly pronounced in museum settings, where dialogues are expected to be informative, engaging, and contextually aware. Visitors may have diverse interests, and the system must adapt its responses accordingly, often with limited explicit feedback.\\

To address these challenges, Hierarchical Reinforcement Learning (HRL) has been shown to be able to compensate for these problems through structures that allow for temporal abstraction and modular policy learning \cite{cuayahuitl2010evaluation}. 

\subsection{Hierarchal Reinforcement Learning}

The scalability issues outlined above arise because a flat agent must decide at
every time step which \emph{primitive} action to take.  As tasks become longer
and rewards sparser, learning such fine-grained control quickly becomes
sample-inefficient and prone to myopic exploration.  
\emph{Hierarchical Reinforcement Learning} (HRL) tackles this
problem by endowing the agent with \emph{temporal abstraction}: beside
elementary actions it may select \emph{macro–actions} (or
\textit{options})—closed-loop policies that unfold over multiple steps
\citep{dayan1993feudal,dietterich2000maxq,sutton1999between}.  
An HRL agent therefore learns a policy \(\pi\) that operates on several
time-scales, delegating short-horizon control to lower layers while higher
layers concentrate on long-term strategy.\\

When actions can last more than one step we
model the environment as a \emph{semi-Markov decision process} (SMDP).  The
one-step transition kernel \(P(s' \mid s,a)\) is replaced by a joint
distribution over next state and duration,
\[
P(s',\tau \mid s,a), \quad \tau\in\mathbb{N}_{+},
\]
so that planning and learning algorithms generalise naturally while accounting
for the elapsed time \(\tau\).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{Images/Introduction/HRLintro.png}
    \caption{Hierarchical learning dynamics.  
    A high-level policy \(\pi\) selects a macro-action
    \(\sigma\) (black arrow) spanning several primitive
    actions \(a\); the low-level policy \(\pi_{\sigma}\) controls those
    primitives.  Value estimates at both levels (\(V_{\pi}\) in green and
    \(V_{\sigma}\) in grey) propagate credit over different temporal
    horizons.  Adapted from \citet{ribas2011neural}.}
    \label{fig:hrl_dynamics}
\end{figure}

Figure~\ref{fig:hrl_dynamics} illustrates how hierarchical control
factors credit assignment: the green returns \(V_{\pi}\) bridge long
gaps between decisive moments, while grey returns \(V_{\sigma}\) guide
the execution of the macro-action itself.\\


\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{Images/Introduction/Hiearach.jpg}
    \caption{Conceptual illustration of Hierarchical Reinforcement Learning. The diagram showcases the interaction between high-level policies (options) and low-level actions, emphasizing temporal abstraction and modular decision-making. Adapted from \citet{bougie2022hierarchical}.}
    \label{fig:hierarchal}
\end{figure}

Since the 1990s a series of foundational algorithms—Feudal RL
\citep{dayan1993feudal}, MAXQ decomposition \citep{dietterich2000maxq},
and the Options framework \citep{sutton1999between}—have shown that
temporal abstraction affords three principal advantages:

\begin{itemize}
    \item \textbf{Long-term credit assignment:} extended actions collapse
          delayed rewards onto higher-level decisions, accelerating learning.
    \item \textbf{Structured exploration:} sampling entire sub-policies, rather
          than isolated primitives, enables coherent exploration over sparse
          reward landscapes.
    \item \textbf{Knowledge transfer:} reusable options capture task
          sub-structure and can be ported across domains or fine-tuned
          with little additional data.
\end{itemize}

These properties make HRL a natural candidate for complex, open-ended dialogue—where an agent must juggle immediate conversational moves with overarching curatorial or pedagogical goals. Before surveying contemporary systems, we briefly recap the \emph{Options} framework that operationalizes temporal abstraction in our setting and fixes notation for the remainder.

\subsubsection{Options Framework}

The most well known HRL method is known as the options framework by \cite{sutton1999between}. In the HRL options framework, an agent selects high-level actions (options), each of which encapsulates a policy $\pi_o$, an initiation set $\mathcal{I}_o$, and a termination condition $\beta_o$. An \textit{option} $o$ is defined as a tuple:
\[
o = (\mathcal{I}_o, \pi_o, \beta_o)
\]
- $\mathcal{I}_o \subseteq \mathcal{S}$: the set of states where the option can be initiated,
- $\pi_o(a|s)$: the intra-option policy,
- $\beta_o(s) \in [0,1]$: the probability the option terminates in state $s$.\\

The value function for a policy over options $\pi$ becomes:
\[
V^\pi(s) = \mathbb{E} \left[ \sum_{k=0}^{\infty} \gamma^{\tau_k} r_k \mid s_0 = s, \pi \right]
\]
where $\tau_k$ is the duration of the $k$-th option execution.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{Images/Introduction/OPTIONS.png}
    \caption{Visualization of different temporal granularities: primitive actions in MDPs (top), temporally abstract transitions in SMDPs (middle), and options over MDPs (bottom). Adapted from \citet{sutton1999between}.}
    \label{fig:options}
\end{figure}

Figure~\ref{fig:options} illustrates the conceptual advantage of HRL, and specifically the options framework. HRL with options enables structured transitions between these levels: the agent selects an option, which then executes a sequence of low-level actions internally until a termination condition is met.\\

The options framework adds helpful modularity to the system. By structuring decisions into higher-level options and lower-level actions, the agent can explore more effectively over longer stretches of the conversation while reusing sub-policies, which makes learning more efficient. These learned options can often transfer to new tasks or domains without starting from scratch, and the clear structure of the hierarchy also makes it easier to understand and analyze what the agent is trying to achieve at each step.\\

In dialogue systems, these advantages materialize as high-level conversational strategies (e.g., \emph{introduce exhibit}, \emph{answer question}, \emph{guide user}) realized by lower-level utterance choices. With this foundation in place, we now turn to an up-to-date review of four strands that shape our design: (i) hierarchical control for extended dialogues, (ii) gaze-based engagement sensing, (iii) intent features as compact state, and (iv) structure-aware language generation.

\subsubsection{Hierarchical control and learning for long, coherent conversations}


Hierarchy in dialogue means the agent separates \emph{when} to change strategy from \emph{how} to realize utterances while a strategy is active. Concretely, we model four strategic options (Explain, AskQuestion, OfferTransition, Conclude) and let a high-level policy over options decide which one to start in state $s_t$, while a low-level policy executes utterances until the option stops. This is precisely the setting of options over SMDPs \citep{sutton1999between}: a manager samples $o_t \sim \mu(o\mid s_t)$, the active option follows an intra-option policy $a_{t+k}\sim \pi_{o_t}(\cdot\mid s_{t+k})$ for $k=0,\ldots,\tau-1$, and a termination function $\beta_{o_t}(s_{t+k})\in[0,1]$ decides—at each intermediate state—whether control returns to the manager. If $\tau$ is the (stochastic) duration until termination, the option-value is:
\[
Q_U(s_t,o_t)=\mathbb{E}\!\left[\;\sum_{k=0}^{\tau-1}\gamma^{k}r_{t+k} \;+\; \gamma^{\tau} V(s_{t+\tau}) \;\middle|\; s_t,o_t\right],
\]
which collapses the rewards accrued over several turns onto a single strategic commitment and then backs off to the continuation value $V(s_{t+\tau})$ when the option ends. This SMDP return is what improves long-horizon credit assignment in conversation: delayed signals (engagement, learning, novelty) attach to a few decisive option choices rather than to every primitive turn.\\

For completeness, we will also refer to two values used in the Option--Critic literature: the value of \emph{starting} option $o$ in $s$,
\[
Q_O(s,o)\;=\;\mathbb{E}\!\left[\sum_{k=0}^{\tau-1}\gamma^{k}r_{t+k}+\gamma^\tau V(s_{t+\tau})\;\middle|\;s_t=s,o_t=o\right],
\]
and the value of taking a primitive action $a$ \emph{while} option $o$ is active,
\[
Q_U(s,o,a)\;=\;\mathbb{E}\!\left[r(s,a)+\gamma\,\bigl((1-\beta_o(s'))\,Q_O(s',o)+\beta_o(s')\,V(s')\bigr)\right].
\]
These are consistent with the SMDP return above and let us write clean learning signals (advantages) later on.\\

Two coupled learning problems follow directly from this structure. \emph{Between-option learning} fits the manager $\mu(o\mid s)$ that selects the next strategic move; in the case of Conversational Agent in a museum, $s$ can include user intent cues and user interest metrics, so the manager can prefer options that relate to user focus and what they specfiically say allowing the agent to pivot to more appropriate options when metrics like attention or intent shift. \emph{Within-option learning} fits each option’s intra-option policy $\pi_o(a\mid s)$—how to realize turns while $o$ is active—and its termination $\beta_o(s)$—when to stop and hand control back. Termination governs pacing. A good option encompassing a conversational strategy like explaining should continue across multiple turns while dwell remains high and end quickly when dwell drops or the visitor asks a question. Option--Critic \citep{bacon2017option} is designed for this: it provides SMDP-consistent gradients to learn both $\pi_o$ and $\beta_o$ from interaction, rather than fixing option length by hand.\\

Formally, the duration $\tau$ is induced by the state-dependent stopping hazard:
\[
\Pr(\tau=n\mid s_t,o_t)\;=\;\Biggl[\prod_{k=0}^{n-1}\bigl(1-\beta_{o_t}(s_{t+k})\bigr)\Biggr]\;\beta_{o_t}(s_{t+n}),
\]
so learning $\beta_o$ is tantamount to learning the distribution of option lengths. Using option-level and intra-option advantages
\[
A_O(s,o)=Q_O(s,o)-V(s),\qquad A_U(s,o,a)=Q_U(s,o,a)-Q_O(s,o),
\]
the Option--Critic updates take the standard policy-gradient form:
\[
\nabla_{\theta_\mu} J \;\approx\; \mathbb{E}\!\left[\nabla_{\theta_\mu}\log \mu(o_t\!\mid s_t)\;A_O(s_t,o_t)\right],\qquad
\nabla_{\theta_o} J \;\approx\; \mathbb{E}\!\left[\nabla_{\theta_o}\log \pi_o(a_t\!\mid s_t)\;A_U(s_t,o_t,a_t)\right],
\]
and the termination gradient controls pacing:
\[
\nabla_{\vartheta_o} J \;\approx\; \mathbb{E}\!\left[-\,\nabla_{\vartheta_o}\beta_o(s_t)\;A_O(s_t,o)\right].
\]
When $A_O>0$ (staying in the option beats handing control back), this pushes $\beta_o(s)$ down (longer runs); when $A_O<0$, it raises $\beta_o(s)$.\\

This formal split mirrors the practical needs of a museum guide. The manager chooses a strategy at a cadence that matches the visitor’s focus and goals; the option then handles utterance-level realization and decides when that strategy has run its course. In symbols, the continuation probability inside an option is $1-\beta_o(s_{t+k})$, so the termination time $\tau$ has a state-dependent hazard; longer $\tau$ under sustained attention increases $\sum_{k<\tau}\gamma^k r_{t+k}$, while early termination shortens exposition and frees the manager to switch strategy. Because $\gamma^\tau$ scales the bootstrap to $V(s_{t+\tau})$, choosing \emph{appropriate} $\tau$ is an integral part of credit control rather than an afterthought.\\

Empirically, hierarchical control has delivered consistent gains in dialogue. Feudal decompositions route decisions through a manager--worker split, reducing the branching factor per decision and scaling multi-domain managers \citep{dayan1993feudal,casanueva2018feudal}. MAXQ shows that decomposing tasks into sub-tasks yields reusable structure and better exploration \citep{dietterich2000maxq}. For end-to-end task systems, HDNO couples a high-level controller over latent dialogue acts with a low-level surface realizer under the options view and improves task success and human ratings on MultiWOZ compared to flat baselines \citep{wang2021hdno}. In composite task completion, learning around sub-goals simplifies exploration and improves sample efficiency when rewards are sparse or delayed \citep{peng2017composite}, and subgoal discovery can further automate the hierarchy for large action spaces \citep{tang2018subgoal}. Open-domain settings also benefit: hierarchical policies improve coherence and goal completion by letting a high-level controller steer local generation \citep{saleh2020hierarchical}. More recently, guidance signals and offline structure have been used to stabilize or pre-shape hierarchical policies at scale—human-in-the-loop pruning and evaluation to regularize option choices across domains \citep{rohmatillah2023hrl_guidance}, and simulation-free latent policy planning that discovers fine-grained skills from logs before online finetuning \citep{he2025ldpp}.\\

Taken together, these results can help us motivate a pattern to adopt: an interpretable option set whose manager $\mu$ is trained to read engagement and intent, with each option learning both when to engage a conversational strategy ($\pi_o$) and when to stop ($\beta_o$). In our setting, that means a conversational strategy like explaining persists while engagement metrics are strong; termination increases when attention plateaus or the visitor’s intent shifts; and a strategy associated with boosting engagement like asking a question can then take over. This is the mechanism by which hierarchy delivers long-horizon coherence without sacrificing local responsiveness.\\

\subsubsection{Engagement sensing with gaze: signals, robustness, and use}

In-gallery engagement must be inferred as the interaction unfolds and without intrusive probes. Eye tracking is well suited to this requirement because it delivers a continuous, unobtrusive record of visual attention that has been repeatedly associated with on-task focus and cognitive–affective involvement in immersive learning settings \citep{Bozkir2021,Dubovi2022,Mikhailenko2022}. Among available gaze measures, dwell time on the currently relevant Area of Interest (AOI) stands out as a dependable approximation to engagement for object-centered experiences. Conceptually, dwell aggregates micro-movements (fixations and interleaved small saccades) into a single proportion of time that the gaze remains within the AOI for a short analysis window aligned to conversational turns. Formally, with $W_t$ a rolling window and $\mathcal{T}_{\text{AOI}}$ the set of timestamps fixated within the exhibit AOI, the online dwell ratio is\\
\[
\text{dwell}_t \;=\; \frac{\lvert \mathcal{T}_{\text{AOI}} \cap W_t \rvert}{\lvert W_t \rvert}\in[0,1],
\]

optionally smoothed across adjacent windows to dampen transient saccades. Methodological syntheses emphasize that AOI-based dwell is comparatively robust to small calibration drift and sampling noise, provided AOIs are defined consistently and windowing is reported \citep{holmqvist2011eyetracking,orquin2018}. In classroom-scale VR, dwell tied to object-of-interest has shown face validity and convergent validity as a marker of on-task attention, tracking when learners remain focused on the pedagogically relevant target \citep{Bozkir2021}; multimodal studies of VR learning likewise report that sustained looking relates to cognitive and emotional engagement \citep{Dubovi2022,Mikhailenko2022}. Together, these findings support dwell as a stable, turn-grained indicator of whether the current exhibit retains attention.\\

Fixation-based summaries offer a complementary but more method-sensitive view of engagement. Metrics such as fixation rate and mean fixation duration have been linked to levels of cognitive processing and load in immersive tasks \citep{Kim2023,Wang2023}, yet their reliability depends on algorithmic choices in fixation detection (velocity/dispersion thresholds) and on eye tracker characteristics, which can introduce between-study variability \citep{holmqvist2011eyetracking}. As a result, fixation dynamics are informative for characterizing the \emph{quality} of looking—sustained scrutiny vs.\ fleeting glances—but warrant careful reporting of detection parameters and sampling rates to ensure interpretability across deployments.\\

A further dimension is the spatial distribution of gaze, often summarized by entropy or dispersion indices. Gaze entropy quantifies how concentrated or scattered visual attention is across competing AOIs; higher entropy typically reflects scanning, divided attention, or exploratory search, whereas lower entropy accompanies focused scrutiny \citep{Gugerty2017}. Recent interactive- and VR-based studies extend this interpretation, showing that entropy tracks attentional shifts and elements of cognitive load, albeit with sensitivity to window size and AOI granularity \citep{Nguyen2024,Lee2024}. This sensitivity underscores a measurement caution: dispersion measures are powerful for describing attentional organization and longer-run trends, but they are noisier at very short horizons and must be interpreted in light of the underlying segmentation.\\

Finally, there is support for the validity of these measures in museum-adjacent contexts. Mobile eye-tracking studies that pair guidance with on-the-move viewing demonstrate that gaze can be captured and aligned with content outside of laboratory conditions \citep{mokatren2016listen}, and broader HRI reviews highlight social gaze as a reliable channel through which attention and interest are manifested in interactions with artificial agents \citep{admoni2017social}. Taken together, the literature positions AOI dwell as a primary, robust indicator of moment-to-moment engagement in object-centered experiences, with fixation dynamics and dispersion as complementary descriptors that enrich—but do not supplant—the core attentional signal provided by dwell.

\subsubsection{Intent features as compact, turn-aligned signals}

Effective turn-by-turn decision making requires distilling raw utterances into a small set of features that say what the visitor is doing \emph{now} and how certain that reading is. Classic pipelines used dialogue acts or domain-specific intent labels to turn open text into policy-ready observations, improving downstream control in noisy, real-time settings \citep{williams2007pomdp,young2013pomdp}. Two shifts make these signals far more reliable in multi-turn museum conversations: (i) contextual encoders that interpret an utterance in light of speaker role and local history, and (ii) pretrained language models with light task heads that learn robust intent cues with modest supervision. DialogueBERT exemplifies the first trend, encoding turn order and roles to lift multi-turn intent recognition over non-contextual baselines, while few-shot variants such as IntentBERT show strong transfer across intent sets with limited labeled data \citep{zhang2021dialoguebert,zhang2021intentbert}. Figure~\ref{fig:dialoguebert-compact-intent} sketches this idea: DialogueBERT encodes each utterance with token, position, speaker-role, and turn-order signals, then uses lightweight task heads to output compact intent features with calibrated confidence. This makes the ‘now’ state explicit and policy-ready without relying on long, brittle text histories.\\

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\linewidth]{Images/LitReview/DialogueBERTArchitecture.png}
  \caption{Turn- and role-aware intent encoding. DialogueBERT augments a pretrained encoder with token, position, \textbf{role}, and \textbf{turn} embeddings and lightweight task heads, producing compact, calibrated features suitable for turn-by-turn control \citep{zhang2021dialoguebert}.}
  \label{fig:dialoguebert-compact-intent}
\end{figure}


Tools like DialogueBERT typically have a few outputs, usually a coarse act/intent \emph{with calibrated confidence}; a cue to question type (e.g., \emph{why} vs.\ \emph{what}) that predicts explanatory shape; a short-history embedding that tracks unresolved topics; and light entity grounding so references could be aligned to the exhibit knowledge base rather than surface strings. These features could map to policy choices and conversational strategies such as \emph{explain}, \emph{probe}, \emph{clarify}, or \emph{transition}. Evidence from task-oriented dialogue confirms that small, well-chosen state summaries improve response selection, error recovery, and turn-taking compared with text-only policies and rule trees \citep{kwan2023dplsurvey,casanueva2018feudal,wang2021hdno}.\\

Empirical results support both effectiveness and efficiency. On fine-grained single-domain data (e.g., BANKING77), intent models built on pretrained encoders achieve strong accuracy with compact representations \citep{casanueva2020banking77}. In multi-turn settings, explicitly encoding role and turn structure yields consistent gains over single-turn baselines \citep{zhang2021dialoguebert}. Industry-facing studies report further improvements when lightweight retrieval or context fusion is added on top of a compact state (average multi-point lifts without heavy supervision), reinforcing the value of concise, context-aware features over raw text alone \citep{liu2024lara}.\\

 Given this, Intent classifiers degrade under domain shift and when users produce unseen or overlapping intents; long-tailed labels and multi-intent utterances amplify this \citep{larson2019evaluating,casanueva2020banking77}. Multi-turn OOD detectors that leverage short dialogue history outperform single-turn detectors, suggesting that even a thin slice of context helps separate unfamiliar intents from in-scope variants \citep{lang2024caro}. The practical takeaway is to treat intent features as \emph{descriptive inputs with uncertainty}, not rigid gates: pair compact, turn-aligned signals with confidence or entropy and let the policy decide when to trust them or defer to other cues \citep{zhang2021intentbert}.

\subsubsection{Synthesis and open gap}
\label{sec:synthesis-gap}

The earlier pieces fit together cleanly. Hierarchical control supplies the long-range backbone for conversation: a manager selects a strategy (an option) that can persist across several turns, while an intra-option policy realizes the utterances until a learned termination returns control \citep{sutton1999between,bacon2017option}. Prompt-engineered headers then sit at the realization boundary: each option (\textsc{Explain}, \textsc{AskQuestion}, \textsc{OfferTransition}, \textsc{Conclude}) owns a short, human-readable template with slots, paired with a compact context vector, so the language model can speak naturally but stay aligned with the active strategy \citep{robino2025conversation,wang2024planning}. The manager’s state is small and actionable: turn-aligned intent cues (role- and history-aware encoders) indicate what the visitor is doing now and with what confidence, and gaze dwell over the relevant object acts as a robust online proxy for attention in object-centered settings \citep{zhang2021dialoguebert,zhang2021intentbert,holmqvist2011eyetracking,orquin2018,Bozkir2021,Dubovi2022}. Together, these signals decide when to persist with a strategy and when to switch, while the header keeps surface language fluent and on-style.\\

This pattern is consistent with results in dialogue and planning: decompositions reduce branching and improve multi-domain control; coupling a high-level controller over latent acts with a low-level realizer improves task success and human ratings on MultiWOZ; open-domain variants report better global coherence when a controller steers local generation \citep{dietterich2000maxq,casanueva2018feudal,wang2021hdno,saleh2020hierarchical}. It also mirrors grounded hybrids where an external controller keeps a fluent generator on task without strangling expressiveness \citep{ahn2022saycan}. Training methods map directly: actor--critic with semi-MDP returns focuses credit on option choice and termination (Option--Critic), while SAC, or conservative Q-learning cover on-policy, off-policy regimes; even modest RL on the decision layer improves multi-turn robustness \citep{bacon2017option,schulman2017ppo-openreview,haarnoja2018sac,kumar2020conservative,su2017sample,weisz2018sample}.\\

Prior museum agents established the value of guidance but relied heavily on hand-authored rules and fixed pacing \citep{kopp2005conversational,traum2012ada}. Contemporary work demonstrates each ingredient in isolation—hierarchy, intent features, prompt schemas, or gaze-driven engagement—but there is little evidence for a \emph{unified, option-based controller that (i) learns when to switch strategies from online engagement (dwell) and intent cues, and (ii) realizes each strategy with a slot-filled prompt header so surface language remains flexible and natural}. In short: \emph{learned timing, compact state, and prompt-level realization have not been evaluated together in situated, educational dialogue.}\\

This thesis will implement and test that integration. The manager will read a compact state (intent features with calibrated confidence; AOI dwell and simple gaze diagnostics; short history) and choose among a small, interpretable option set. Each option will expose a prompt header with slots (goal sketch, constraints such as time budget/reading level, KB entities), which the LLM fills to produce the next turn. Termination will be learned, not fixed, so pacing adapts to attention and intent shifts in real time. Evaluation will compare against strong non-hierarchical baselines and ablations (no gaze; no intent; fixed-duration options; header-only without a manager), with outcomes on multi-turn coherence and content coverage, engagement (dwell trajectories), and task success. If successful, the result would show that strategic planning and learned timing deliver the long-horizon structure, while prompt headers let large language models do what they do best locally: produce high-quality, flexible language that stays on plan.

\section{Research Questions}
\label{sec:research-questions}

This thesis explores whether a small hierarchy can keep museum conversations organized and adaptive, while a language model keeps the wording natural. We use hierarchical reinforcement learning with options and learned stopping rules to decide what strategy to use and when to switch, and compact prompt headers to guide how to say the next turn.\\

\noindent\textbf{Main research question}\\

To what extent is a hierarchical reinforcement learning approach (options with learned termination, plus structured prompt headers) a justified design choice for museum conversational agents—that is, can a proof-of-concept system show measurable control of long-horizon engagement, content coverage, and adaptive pacing while remaining practical to operate and maintain?\\

To answer this main question, we pose three supporting questions that test the core parts of the approach:\\

\textbf{RQ1: Does an option-based manager maintain long-horizon coherence and keep turns aligned with the intended strategy more reliably than a flat policy under the same prompts and rewards?}\\
  
\noindent This asks whether a small set of high-level strategies (Explain, Ask, OfferTransition, Conclude) helps the agent stay on plan: continue a strategy for several turns when it makes sense, switch when signals say it should, and avoid drifting or repeating.\\

\textbf{RQ2: Do learned option terminations respond to changes in engagement and intent in the intended direction—switching sooner when attention drops or the visitor’s goal shifts, and persisting during stable, high-engagement spans?}\\
  
\noindent Here we check whether stopping behavior is sensible in practice: longer explain spans during steady dwell, and faster switches to questions or transitions when dwell falls or a new intent appears.\\

\textbf{RQ3: Do compact, slot-filled prompt headers tied to the active option improve local realization—faithfulness to the chosen move, grounding to the exhibit knowledge base, and non-redundant phrasing—relative to unconstrained generation?}\\
  
\noindent This focuses on wording quality: do short headers that state the current option, subaction, focus, last user utterance, and allowed facts help the model say the next thing clearly and on-topic compared with unconstrained generation?\\

\noindent The first supporting question tests long-range structure, the second tests timing and adaptivity, and the third tests clarity and grounding at the turn level. Positive results on all three together would show that the hierarchical design achieves the aims of the main question: keeping visitors engaged longer, covering new content with fewer repeats, and adjusting pacing in a way that remains manageable in practice.

\section{Problem Formulation}
\label{sec:problem-definition}

Here we formalize the conversational museum agent as a hierarchical decision process: a high-level policy selects conversational strategies (options) such as \textsc{Explain}, \textsc{AskQuestion}, \textsc{OfferTransition}, and \textsc{Conclude}, while a low-level policy realizes each turn with an LLM guided by compact, slot-filled prompt headers. The hierarchy follows the options framework \citep{sutton1999between} to handle long-range credit assignment, and the state blends gaze-based engagement (AOI dwell), lightweight dialogue history, and turn-aligned intent/context cues. Rewards balance immediate attention with non-redundant content coverage, and training uses an actor–critic approach with SMDP returns—learning both option selection and termination—so pacing and strategy changes are driven by evidence in the state rather than fixed turn limits.

\subsection{Hierarchical Framing}

We model the agent as a two-level \textbf{hierarchical semi-Markov decision process (SMDP)}, where the high-level policy selects communicative strategies (options) such as \textsc{Explain} or \textsc{AskQuestion}, while the low-level policy selects utterances within these strategies, using structured templates to maintain clarity and safety during generation. This structure directly addresses the challenges of long-term credit assignment and exploration in sparse-reward, multi-turn settings noted in HRL literature \citep{bacon2017option, peng2017composite}.

\subsection{State Representation}
\label{sec:state-representation}

We design the agent's state space as a structured vector that captures both what the visitor is looking at and how they're communicating. At each dialogue turn $t$, the state is defined as:

\[
s_t = [\mathbf{f}_t, \mathbf{h}_t, \mathbf{i}_t, \mathbf{c}_t]
\]

Where each component captures a different aspect of the interaction:

\begin{itemize}
  \item \textbf{Focus Vector $\mathbf{f}_t$:} Tells us which exhibit the visitor is currently looking at. For 8 exhibits, we define:
  \[
  \mathbf{f}_t = [f_{t,1}, f_{t,2}, \ldots, f_{t,8}, f_{t,\emptyset}]
  \]
  where exactly one $f_{t,i} = 1$ (the exhibit they're focused on) and all others $= 0$. The dimension is $9$.
  
  \item \textbf{Dialogue History Vector $\mathbf{h}_t$:} Keeps track of what we've already discussed:
  \[
  \mathbf{h}_t = [\mathbf{h}_t^{exh}, \mathbf{h}_t^{act}]
  \]
  where $\mathbf{h}_t^{exh}$ records which exhibits we've explained (1 = explained, 0 = not yet), and $\mathbf{h}_t^{act}$ tracks how often we've used each dialogue strategy (Explain, AskQuestion, etc.). For 8 exhibits and 4 strategies, the dimension is $12$.

  \item \textbf{Intent Embedding $\mathbf{i}_t$:} Understands what the visitor is trying to communicate using DialogueBERT \citep{zhang2021dialoguebert} specifically in the last turn. For each visitor utterance in the last turn $u_t$, we compute the 768-d pooled representation and keep the state compact by projecting it with a fixed, offline-trained linear map $\mathbf{P}\in\mathbb{R}^{64\times 768}$:
  \[
  \mathbf{e}_t = \text{DialogueBERT}(u_t,\text{role}=\text{"user"})\in\mathbb{R}^{768},\qquad
  \mathbf{i}_t = \mathbf{P}\mathbf{e}_t \in \mathbb{R}^{64}.
  \]
  DialogueBERT encodes turn and speaker-role context and reports strong performance on multi-turn understanding, aligning with our use case \citep{zhang2021dialoguebert}.

  \item \textbf{Dialogue Context $\mathbf{c}_t$:} Remembers the recent conversation to avoid repeating ourselves or contradicting what we just said. We encode the last 3 exchanges by averaging their pooled vectors and applying the same projection $\mathbf{P}$:
  \[
  \mathbf{c}_t = \mathbf{P}\cdot \tfrac{1}{3}\sum_{k\in\{t-2,t-1,t\}}\text{DialogueBERT}_{\text{[CLS]}}(u_k) \in \mathbb{R}^{64}.
  \]
\end{itemize}

\noindent\textbf{Total state dimension (8 exhibits):} $9$ (focus) $+\,12$ (history) $+\,64$ (last-turn semantics) $+\,64$ (short context) $= \mathbf{149}$.\\

\noindent\textbf{State dimension scaling:} For $n$ exhibits, the total dimension is $2n + 5 + 128 = 2n + 133$. The formula breaks down as: focus vector ($n+1$), history vector ($n+4$), intent embedding ($64$), and dialogue context ($64$). This design scales linearly with the number of exhibits while keeping the semantic embeddings fixed at 128 dimensions total.\\

The last-turn vector $\mathbf{i}_t$ is a compact “semantic coordinate” for $u_t$ (64 numbers that preserve the main meaning cues from the original 768-d space), while $\mathbf{c}_t$ carries a small, separate summary of very recent dialogue (another 64 numbers). Keeping these as two small pieces lets the policy read both “what’s happening now” and “what just happened” without handling the full high-dimensional encoder output. Reducing 768$\rightarrow$64 is a standard move: compact sentence embeddings (e.g., SBERT/MiniLM at 384-d) retain strong task performance \citep{reimers2019sbert,wang2020minilm}, and distance-preserving projections are known to maintain the geometry policies depend on for decision boundaries in much lower dimensions (e.g., random/PCA projections under Johnson–Lindenstrauss–type guarantees).\\

The DialogueBERT integration solves several key problems in museum dialogue systems: (1) \textbf{Better understanding} of diverse visitor communication styles, from direct questions to contemplative observations; (2) \textbf{Conversation memory} that prevents repetitive explanations and maintains coherent narrative flow; (3) \textbf{Adaptive responses} that work across different visitor demographics and communication patterns; and (4) \textbf{Interpretable behavior} through semantically meaningful representations that help us understand and improve the system.\\

This state design enables the agent to learn sophisticated dialogue strategies that feel natural and engaging while efficiently covering educational content. Realization details (dialogue action tokens, prompt headers, and KB grounding) are specified in \ref{sec:action-grounding}.


\subsection{Action Space}
\label{sec:action-space}

We model actions as options over subactions. The high-level policy chooses one of four options; within an option, the intra-option policy selects a subaction. Each subaction has a clear behavioral contract and typical effects on the state (e.g., knowledge coverage, engagement). \textbf{For \textsc{OfferTransition}, we consolidate behavior into a single subaction that (i) assesses what has been discussed so far and (ii) offers a transition to the exhibit with the most remaining unexplained facts.} Table~\ref{tab:actions} summarizes the subactions we implement in code.

\begin{landscape}
\begin{table}[H]
\centering
\small
\begin{adjustbox}{max width=\linewidth}
\begin{tabularx}{\linewidth}{l l Y Y}
\toprule
\textbf{Option} & \textbf{Subaction (token)} & \textbf{What it does (contract)} & \textbf{Primary effect} \\
\midrule
\textsc{Explain} & \textit{ExplainNewFact} 
& Introduce a \emph{new}, KB-backed fact about the focused exhibit; directly tie to \texttt{LAST\_UTT}; cite fact ID. 
& Increases content coverage; may raise dwell if matched to interest. \\
& \textit{RepeatFact} 
& Restate a previously shared fact using simpler/different wording; connect to current question. 
& Reinforcement; reduces confusion after missed points. \\
& \textit{ClarifyFact} 
& Explain a recently used fact more clearly (definitions, examples, analogies). 
& Lowers uncertainty; prepares for progression. \\
\midrule
\textsc{AskQuestion} & \textit{AskOpinion} 
& Solicit the visitor’s view (e.g., style, symbolism) grounded in focus and \texttt{LAST\_UTT}. 
& Elicits engagement signal; surfaces preferences. \\
& \textit{AskMemory} 
& Gentle recall of one–two recent facts to consolidate learning. 
& Checks retention; informs pacing. \\
& \textit{AskClarification} 
& Ask the visitor to clarify an ambiguous request or comment. 
& Reduces misalignment; avoids wrong-depth explanations. \\
\midrule
\textsc{OfferTransition} & \textit{SuggestMove}
& Assess conversation history and KB coverage (facts used vs.\ available) across exhibits; select exhibit $e^\*$ with the \emph{most remaining unexplained facts} and offer a concrete transition rationale
& Changes focus; prioritizes content novelty/coverage; prevents stagnation. \\
\midrule
\textsc{Conclude} & \textit{WrapUp} 
& Warm closing; thank the visitor. 
& Ends session gracefully. \\
\bottomrule
\end{tabularx}
\end{adjustbox}
\caption{Action space semantics used by the planner (rotated for readability).}
\label{tab:actions}
\vspace{0.25em}
{\footnotesize\emph{Note:} Realization with headers and KB IDs is detailed in \ref{sec:action-grounding}. \par}
\end{table}
\end{landscape}

The planner takes the chosen \texttt{option} and \texttt{subaction} from the manager, builds a small shared context (focus exhibit, \texttt{LAST\_UTT}, brief history, facts used/available, KB ID guardrails), and then hands off to the matching subaction builder. Each builder has a simple job contract: \textit{ExplainNewFact} must pick a new fact ID; \textit{RepeatFact} and \textit{ClarifyFact} operate only over facts already used; \textit{AskOpinion}, \textit{AskMemory}, and \textit{AskClarification} ask questions that respond directly to \texttt{LAST\_UTT}; \textit{SuggestMove} \emph{computes coverage across exhibits, selects the exhibit with the highest remaining unexplained content, and proposes that move with a brief justification}; and \textit{WrapUp} closes the session. The short header makes the generator aware of the intended move before decoding, while the HRL layer decides when to keep or switch that header through its learned termination rule.


\subsection{Reward Function Design}
\label{sec:reward-design}

The reward function is designed to align the agent's behavior with the goals of maintaining visitor engagement while efficiently delivering diverse, relevant information about the exhibits.

\paragraph{Engagement Reward $r^{\text{eng}}_t$:}  
We use the visitor's gaze dwell time on the currently focused exhibit as a direct engagement signal:
\[
r^{\text{eng}}_t = \text{dwell}_t
\]
where $\text{dwell}_t \in [0, 1]$ measures the proportion of the previous turn the visitor spent looking at the exhibit, leveraging its robustness as an engagement metric in VR learning contexts \citep{Bozkir2021, Dubovi2022, Mikhailenko2022}.

\paragraph{Knowledge Novelty (incremental) $r^{\text{nov}}_t$:}
We reward introducing new facts via the \emph{increment} in covered facts:
\[
r^{\text{nov}}_t \;=\; \alpha \,\Big(|\mathcal{F}_{\text{used}}(t)| - |\mathcal{F}_{\text{used}}(t-1)|\Big),
\]
with a tuned scale $\alpha$. Separately, we report the cumulative coverage ratio $|\mathcal{F}_{\text{used}}|/|\mathcal{F}_{\text{total}}|$ as an \emph{evaluation metric}.


\paragraph{Combined Reward:}  
The overall reward at each turn is:
\[
R_t = w_e \cdot r^{\text{eng}}_t + w_n \cdot r^{\text{nov}}_t
\]
where $w_e$ and $w_n$ are weights tuned to balance maintaining visitor attention with delivering diverse content.\\

This design ensures that the agent is rewarded for keeping the visitor engaged in real time while progressing through the exhibit content systematically, aligning closely with the pedagogical and interaction goals of the conversational museum agent.


\subsection{Learning Objective}

The agent's objective is to learn a hierarchical policy \(\pi\) that maximizes the expected cumulative reward across a visitor tour:

\[
J(\pi) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{T} \gamma^t R_t \right]
\]



\subsection{Summary}

In summary, this formalization aligns the agent's architecture with the goals of engagement, adaptability, and informative interaction as outlined in our research questions. It leverages HRL to manage dialogue strategy, uses gaze-based metrics for real-time engagement measurement, and employs structured prompting with LLMs to maintain fluency while adhering to pedagogical and conversational goals. This design provides a clear, modular framework for building and evaluating conversational museum agents grounded in the literature while allowing for flexible experimentation in training and evaluation settings. The following sections document how this formalization is realized in practice: Section~\ref{sec:system-architecture} covers the system components (environment, simulator, agent, action execution), and Section~\ref{sec:training-methodology} details the training algorithm and reward extensions.

\section{System Architecture}
\label{sec:system-architecture}

While the formal problem definition establishes the theoretical foundation for hierarchical control in museum dialogue, deploying a working conversational agent requires concrete implementation of the environment, simulator, agent architecture, and action execution mechanisms. This section documents how the abstract formalization is realized in practice, covering the system components that enable training and evaluation. The architecture separates concerns: the environment computes state and rewards, the simulator provides contingent visitor responses, the agent implements hierarchical decision-making, and the dialogue planner translates strategic choices into natural utterances.

\subsection{Environment Implementation}

The environment implements the SMDP formulation from Section~\ref{sec:problem-definition}, providing state computation, reward calculation, and action masking. 

\subsubsection{State Space Computation}

The state vector $s_t = [\mathbf{f}_t, \mathbf{h}_t, \mathbf{i}_t, \mathbf{c}_t]$ is computed as follows. The focus vector $\mathbf{f}_t$ is a one-hot encoding of the current exhibit (or no-focus state). The dialogue history vector $\mathbf{h}_t$ concatenates exhibit completion ratios (facts mentioned per exhibit divided by total facts) and normalized option usage counts. The intent embedding $\mathbf{i}_t$ is computed by projecting the DialogueBERT encoding of the last user utterance through a fixed linear projection matrix $\mathbf{P} \in \mathbb{R}^{64 \times 768}$. The dialogue context $\mathbf{c}_t$ applies the same projection to the average of DialogueBERT encodings from the last three exchanges. This design scales linearly with the number of exhibits: for $n$ exhibits, the total dimension is $2n + 5 + 128 = 2n + 133$.

\subsubsection{Action Space and Masking}

The hierarchical action space consists of four options, each with 2-3 subactions. Action masking enforces dialogue coherence: \textsc{Conclude} is masked until at least 3 facts and 2 exhibits have been covered; \textsc{ExplainNewFact} is masked when all facts for an exhibit are exhausted; \textsc{RepeatFact} is masked when no facts have been mentioned yet. These masks prevent structurally invalid actions and guide the policy toward coherent strategies.

\subsubsection{Reward Calculation}

Rewards are computed per-turn according to the combined reward function (Section~\ref{sec:reward-extensions}). Engagement reward equals dwell time (which may be reduced by simulator-level transition spam penalties). Novelty reward is $0.15 \times |\text{new facts}|$. Responsiveness, transition insufficiency, and conclude bonuses are computed based on dialogue state and action history.

\subsection{Simulator Design: Contingent Visitor Responses}
\label{sec:simulator-implementation}

For training, we use a lightweight simulator instead of live visitors. Each step unfolds as follows: the agent chooses a high-level strategy (an \emph{option}), builds a compact prompt, and a pretrained language model realizes it as a natural sentence. A persona-conditioned user model then replies and also produces a synthetic gaze trace that indicates how much the visitor attended to the current exhibit. When the agent's turn fails to reference anything on display, the simulator yields little or no gaze on the focal object, which we treat as disengagement. This closed loop gives immediate feedback on the two outcomes we care about—moment-to-moment attention (dwell) and progressive coverage of new facts—so the policy can improve.\\

\subsubsection{Simulator Architecture and Response Generation}

Our simulator follows a \textit{persona-conditioned, AOI-grounded, LLM-guided} architecture inspired by recent work on controllable user simulation \citep{shi2019build,zhang2020task}. Each session initializes a visitor persona (Agreeable, Conscientious, Neurotic, based on Big Five traits) and an exhibit focus (current Area of Interest, or AOI). The simulator tracks dialogue history, the agent's last utterance, and the current option/subaction. At each turn, it:

\begin{enumerate}
    \item \textbf{Determines response type} (question, statement, acknowledgment, confusion, follow-up question) via a rule-based classifier conditioned on agent behavior.
    \item \textbf{Generates utterance text} using either template-based expansion or an LLM prompt.
    \item \textbf{Synthesizes gaze features} (dwell time, saccade span, entropy, etc.) conditioned on response type, with dwell serving as the primary engagement signal. In our test case, we operate with dwell time ratio.
\end{enumerate}

The response-type classifier is the core of the contingent feedback mechanism. It implements several heuristics grounded in dialogue coherence and visitor behavior:

\paragraph{Exhibit mismatch detection.} If the agent's utterance mentions an exhibit different from the visitor's current AOI (extracted via keyword and semantic matching), the simulator flags a coherence violation. With 70\% probability, the response type is set to ``confusion,'' yielding low dwell (0.25--0.50) and an utterance like ``Wait, I'm looking at [current exhibit]—why are you talking about [other exhibit]?'' This operationalizes the finding that off-topic responses disrupt engagement \citep{bohus2009ravenclaw,young2013pomdp}.\\

\paragraph{Question deflection detection.} If the visitor asked a question in the prior turn (\texttt{last\_user\_question} is set) and the agent chose \textsc{AskQuestion} (rather than \textsc{Explain}), the simulator detects a deflection. With 60\% probability, the response type is ``confusion,'' with utterances like ``I asked you a question—could you answer it first?'' This creates the training signal for the responsiveness reward: deflection produces low dwell and confused responses, while answering produces acknowledgment or follow-up questions (high dwell). This is consistent with findings that users disengage when systems repeatedly deflect or fail to address explicit requests \citep{jiang2015natural,su2016line}.\\

\paragraph{Quality-contingent positive responses.} When the agent provides a substantive answer (detected via fact IDs \texttt{[ID]} in the utterance, or information-bearing keywords), the simulator probabilistically selects positive response types: ``acknowledgment'' (40\%, e.g., ``Oh, that's fascinating!'') or ``follow-up question'' (60\%, e.g., ``What else can you tell me about that?''). Both yield high dwell (0.75--0.95), creating a reward signal for informative, grounded explanations.\\

\paragraph{Neurotic persona modulation.} The Neurotic persona has a baseline 15\% chance of confusion responses even when the agent behaves well, modeling visitors who are inherently less certain or more questioning. This adds diversity and prevents overfitting to always-positive feedback.

\subsubsection{Gaze Feature Synthesis: Dwell as Contingent Engagement}

Gaze dwell time is the primary engagement signal in our reward function. To ensure it provides a meaningful learning signal, we synthesize dwell values contingent on response type, creating a direct coupling between dialogue quality (as judged by the response classifier) and the engagement reward.\\

For each response type, we define a base dwell range informed by empirical gaze studies in VR and museum settings \citep{Bozkir2021,Dubovi2022}:\\

\begin{itemize}
    \item \textbf{Acknowledgment / Follow-up question} (high engagement): dwell $\in [0.75, 0.95]$, sampled uniformly. These responses indicate the visitor is satisfied and curious, so gaze remains focused on the exhibit.
    \item \textbf{Question} (moderate-high engagement): dwell $\in [0.40, 0.70]$. The visitor is engaged but seeking information, so attention is steady but not peak.
    \item \textbf{Statement} (moderate engagement): dwell $\in [0.30, 0.60]$. Neutral observation; attention is present but less intense.
    \item \textbf{Confusion} (low engagement): dwell $\in [0.25, 0.50]$, with an additional $0.8\times$ penalty multiplier. Confusion signals the agent has lost the visitor, so gaze wanders or drops.
\end{itemize}

These ranges were calibrated during early development to produce a reward signal with sufficient dynamic range: good behavior (answering questions, staying on-topic) yields $r^{\text{eng}}_t \approx 0.7$--0.9, while poor behavior yields $r^{\text{eng}}_t \approx 0.2$--0.4, a difference of 0.3--0.5 per turn. Over a 10-turn dialogue, this compounds to a 3--5 point difference in cumulative return, sufficient to drive policy learning.\\

The dwell values are further modulated by an \textit{engagement level} tracker that decays with repeated off-topic or deflecting behavior. If the agent produces two consecutive low-quality turns (detected via exhibit mismatch or deflection), the engagement level drops by 40\%, and all subsequent dwell values are scaled down until the agent recovers with on-topic, informative responses. This implements a simple model of visitor patience: people tolerate occasional mistakes, but repeated failures lead to disengagement and early exit. Empirically, this mechanism caused poorly performing policies to receive shorter episodes (visitors mentally ``check out'' after 5--6 turns, yielding low returns), while good policies sustained engagement for 12--15 turns, maximizing coverage and conclude bonuses.

\subsubsection{Transition Logic: Probabilistic Acceptance}

As described in the transition control subsection (Section~\ref{sec:reward-extensions}), the simulator models transitions as probabilistic events conditioned on exhibit completion. When the agent selects \textsc{OfferTransition}, the simulator:

\begin{enumerate}
    \item Computes current exhibit completion: $c = |\mathcal{F}_{\text{current}}| / |\mathcal{F}_{\text{total}}|$.
    \item Maps $c$ to acceptance probability $p_{\text{accept}}$ (staged: 0.20 / 0.50 / 0.80 / 0.95).
    \item Draws a Bernoulli random variable; if success, update \texttt{current\_exhibit} to \texttt{target\_exhibit} and select a random AOI from the new exhibit; if failure, remain at current exhibit and set response type to ``confusion.''
\end{enumerate}

On success, the visitor utterance is generated via LLM with a prompt framing: ``The guide suggested visiting [target exhibit]. You're interested and ready to move.'' This yields affirmative responses like ``Sure, let's check that out!'' On failure, the prompt is: ``The guide wants to move on, but you feel you've barely learned anything here. Express confusion.'' This produces utterances like ``Wait, we just got here—can you tell me more about this first?''\\

This probabilistic mechanism teaches the agent two lessons: (i) transitions are more likely to succeed after explaining multiple facts, incentivizing depth before breadth; (ii) transitions can fail even when well-timed, so the agent must handle rejection gracefully (typically by returning to \textsc{Explain}). Empirically, learned policies adapted their transition timing: early in training, agents attempted transitions after 1--2 facts (avg. success rate 35\%); by convergence, they waited for 3--4 facts (avg. success rate 78\%), and learned to explain further after rejections rather than immediately re-attempting transition.

\subsubsection{Utterance Generation: Template and LLM Modes}

For efficiency, the simulator operates in two modes. \textbf{Template mode} (fast) uses hand-authored templates per response type (e.g., ``What's special about the [AOI]?'' for questions, ``That's interesting!'' for acknowledgments), with slot-filling for the current AOI and exhibit. This runs in $<10$ms per turn and is used during high-throughput training. \textbf{LLM mode} (detailed) uses an LLM with a structured prompt that includes the agent's last utterance, current AOI, persona, and desired response type. The prompt instructs the LLM to generate a 1--2 sentence natural response that reflects the visitor's state (engaged, confused, curious). This produces more diverse and contextually appropriate utterances at the expense of longer turn times during training.

\subsection{Agent Architecture}

The agent implements hierarchical decision-making through an Actor-Critic network with option-level and subaction-level policies. The network outputs option probabilities, intra-option subaction probabilities, termination probabilities, and value estimates. Option selection follows the Option-Critic architecture \citep{bacon2017option}: the agent selects an option, executes it for multiple turns (with learned termination), then selects a new option based on the updated state. Action masking constrains available options and subactions based on dialogue state.

\subsection{Action Masking and Strategic Constraints}
\label{sec:action-masking-detail}

To prevent structurally invalid actions and guide the policy toward coherent strategies, we implement action masking at both the option and subaction levels. Masking dynamically adjusts the available action set based on the current dialogue state, effectively encoding domain constraints as hard constraints on the policy \citep{huang2020action,tang2021discovering}.

\subsubsection{Option-Level Masking}

The primary option-level constraint is on \textsc{Conclude}: the agent may not end the session prematurely. We define two thresholds:

\begin{itemize}
    \item \texttt{min\_facts\_before\_conclude = 3}: At least 3 facts must have been mentioned across all exhibits.
    \item \texttt{min\_exhibits\_before\_conclude = 2}: At least 2 distinct exhibits must have been discussed (i.e., have $|\mathcal{F}_e| > 0$).
\end{itemize}

If either condition is unmet, \textsc{Conclude} is masked (unavailable). This prevents the pathological policy of concluding immediately for safety, which occasionally emerged in early unconstrained training (yielding zero novelty and conclude bonus, but avoiding any risk of low dwell from bad explanations—a local minimum).\\

The thresholds are set based on minimal tour expectations: a ``guided tour'' that covers only one exhibit with 1--2 facts is not a tour at all. The values (3 facts, 2 exhibits) represent the bare minimum for a coherent session. Empirically, trained policies typically exceed these thresholds significantly (avg. 8--12 facts, 5--7 exhibits), so the masking acts as a safety guardrail rather than a binding constraint during normal operation.

\subsubsection{Subaction-Level Masking}

Within the \textsc{Explain} option, we mask subactions based on knowledge availability:

\begin{itemize}
    \item \textbf{ExplainNewFact} is masked if all facts for the current exhibit have already been mentioned ($|\mathcal{F}_{\text{current}}^{\text{new}}| = 0$). This prevents the agent from attempting to introduce ``new'' facts when none remain, which would force the LLM to either hallucinate or produce a generic non-explanation.
    \item \textbf{RepeatFact} is masked if no facts have been mentioned yet for the current exhibit ($|\mathcal{F}_{\text{current}}^{\text{used}}| = 0$). This prevents the agent from trying to ``repeat'' something that was never said.
    \item \textbf{ClarifyFact} is always available (no masking), as clarification can be provided even without explicit fact repetition (e.g., defining terms, providing analogies based on prior context).
\end{itemize}

These masks are implemented via \texttt{\_get\_available\_subactions(option)}, which returns a filtered list of subactions. The actor's subaction selection is then clamped to this list: if the actor's raw output would select an invalid subaction, it is replaced with the first available one. This ensures the prompt builder always receives a valid subaction, maintaining the subaction contract.\\

Masking serves two purposes. First, it provides \textit{safety}: the agent cannot produce structurally nonsensical moves (e.g., repeating a fact that was never mentioned). Second, it provides \textit{pedagogical guidance}: by disabling \textit{ExplainNewFact} when the exhibit is exhausted, we force the agent to either ask questions, transition, or conclude—preventing it from ``spinning'' on the same exhibit indefinitely. Studies of hierarchical RL show that task-structure-aware masking improves both sample efficiency and final performance by pruning the action space to semantically valid choices \citep{huang2020action,tang2021discovering}.

\subsection{Summary: System Architecture}

The architecture documented in this section operationalizes the formal hierarchical framework into a practical, trainable dialogue agent. The environment computes state and rewards, the simulator provides contingent feedback that reflects realistic visitor behavior—rewarding good dialogue moves with high engagement and positive responses, penalizing poor moves with confusion and disengagement. The agent implements hierarchical decision-making through Option-Critic architecture, and grounding mechanisms (fact verification, prompt contracts, action masking) ensure the agent stays faithful to the knowledge base and produces structurally coherent dialogue strategies.\\

Together, these components enable the hierarchical policy to learn long-horizon conversational strategies: the manager learns which options to deploy and when to switch based on engagement and intent signals, while intra-option policies learn how to realize those strategies through structured prompting of the LLM. The result is a system that balances the flexibility and fluency of large language models with the coherence and goal-directedness of learned hierarchical control.

\section{Training Methodology}
\label{sec:training-methodology}

While the formal problem definition establishes the theoretical foundation for hierarchical control in museum dialogue, deploying a working conversational agent requires addressing practical challenges that emerge from the dynamics of real interaction. This section documents the key training extensions beyond the base formalization, their empirical motivation from early development, and their grounding in dialogue systems literature. These extensions operationalize three core requirements: (i) cooperative information exchange aligned with visitor needs, (ii) robust training signals from a simulated visitor that responds contingently to agent quality, and (iii) content pacing that respects both pedagogical coverage and visitor attention.

\subsection{Training Algorithm}

We adopt an \emph{actor--critic} scheme tailored to options and semi-Markov timing \citep{sutton1999between,bacon2017option}. Intuitively, the actor decides which strategy to run and how to realize turns within it; the critic scores those decisions over the entire span of the option, not just the next turn. That alignment with temporally extended behavior is exactly what dialogue needs: strategies like \textsc{Explain} or \textsc{AskQuestion} often pay off a few turns later, and actor--critic naturally attributes credit across that window. It also supports learning \emph{when to stop} via a parameterized termination function, so pacing adapts to attention and intent rather than a fixed turn budget.\\

Other optimizers are defensible, but add overhead we don't need. PPO is widely used and steady \citep{schulman2017ppo-openreview}, yet it doesn't intrinsically learn option termination conditions and requires extra work to make functional with SMDPs. SAC is very robust for continuous control \citep{haarnoja2018sac}, but our action space is small and discrete (strategies and subactions). In short, plain actor–critic is a well-cited, stable choice for HRL with options that lets us train the manager, the within-option behavior, and the learned stopping rule under one roof \citep{sutton1999between,bacon2017option}.\\

Training proceeds in short segments. At each cycle, the manager selects an option; the agent then speaks for several turns under that option; and the critic scores the entire chunk with SMDP returns, summing rewards collected during the option and bootstrapping at its boundary with \(\gamma^{\tau}V(s_{t+\tau})\). The manager updates from these segment-level advantages so it learns which strategic choices pay off. While an option is active, its intra-option policy updates from per-turn advantages to improve local realization, and the termination function learns pacing—tending to persist when dwell and topical relevance remain high, and to hand control back sooner when attention drops or the visitor's intent shifts.\\

To keep training steady, we rely on light, well-known stabilizers: normalize advantages, add a small entropy bonus to avoid early collapse, clip gradient norms, and monitor a gentle KL to flag overly large policy moves. In practice this yields smooth learning curves (low variance), sensible option durations, and fewer oscillations between strategies.\\

Beyond cumulative reward, we report training stability (monotone trend, lower variance), sensible pacing (longer options when dwell is high, earlier switches when it falls), knowledge coverage (higher fraction of novel facts with fewer repeats), and conversation smoothness (fewer thrashy strategy flips, longer coherent stretches). These HRL-relevant diagnostics show the hierarchy is doing useful work, not just overfitting to the simulator.\\

Overall, actor--critic provides a simple, well-grounded path to train \emph{all} moving parts---strategy choice, within-strategy realization, and timing---under the options framework. It is widely used, stable for semi-Markov decisions, and matches how conversational strategies actually unfold over multiple turns \citep{sutton1999between,bacon2017option,schulman2017ppo-openreview,haarnoja2018sac,kumar2020conservative}.

\subsection{Reward Function Extensions}
\label{sec:reward-extensions}

The base reward function in the formal problem definition combines engagement (dwell time) and knowledge novelty (coverage ratio). However, early training runs revealed three systematic failure modes that required additional shaping: agents deflecting visitor questions with counter-questions rather than providing information, premature transitions before sufficient content delivery, and sessions ending with shallow coverage. We address these through three theoretically grounded reward extensions.

\subsubsection{Responsiveness Reward: Cooperative Question Answering}

Conversational cooperation requires that speakers provide information appropriate to the current exchange \citep{grice1975logic}. Grice's Maxim of Quantity specifies that contributions should be ``as informative as required'' and not less so. In museum dialogue, this principle manifests concretely: when a visitor asks a question, they expect substantive information, not deflection. Early training logs showed agents exploiting the \textsc{AskQuestion} option to avoid committing to explanations, producing exchanges like:

\begin{quote}
\textit{Visitor:} ``What materials were used in this painting?''\\
\textit{Agent:} ``What do \emph{you} think the materials might be?''
\end{quote}

This behavior maximizes short-term safety (avoiding potentially incorrect statements) but violates cooperative norms and frustrates information-seeking visitors. Studies of task-oriented dialogue confirm that deflection strategies—particularly counter-questions when the user has explicitly requested information—correlate with lower satisfaction and task abandonment \citep{jiang2015natural,su2016line}. In educational contexts, timely information provision when learners signal readiness has been shown to improve both engagement and learning outcomes \citep{koedinger2012knowledge,rollinson2014framework}.\\

We operationalize this through a \textbf{responsiveness reward component} $r^{\text{resp}}_t$ that directly incentivizes answering questions:

\[
r^{\text{resp}}_t = 
\begin{cases}
+0.25 & \text{if visitor asked question at } t{-}1 \text{ and agent provided new facts at } t \\
-0.15 & \text{if visitor asked question at } t{-}1 \text{ and agent used AskQuestion at } t \\
0 & \text{otherwise}
\end{cases}
\]

The positive component ($+0.25$) rewards using the \textsc{Explain} option with new knowledge-graph facts when the visitor's prior turn was classified as a question by the response-type detector. The negative component ($-0.15$) penalizes deflection: choosing \textsc{AskQuestion} when the visitor has explicitly signaled an information need. The asymmetry (larger positive than negative) reflects that providing information is the primary goal, while deflection is situationally acceptable (e.g., genuine clarification needs) but should not become the default strategy.\\

This mechanism aligns with findings from dialogue policy learning: reward shaping that explicitly encodes task-level constraints (here, answer questions) improves both sample efficiency and final performance compared to end-of-dialogue rewards alone \citep{peng2017composite,su2016line}. The implementation tracks \texttt{\_last\_user\_response\_type} from the simulator's response classifier; when set to ``question'' and the agent's utterance contains validated fact IDs from the knowledge graph, the positive reward triggers. Conversely, when \texttt{option == "AskQuestion"} in a question context, the penalty applies. Empirically, this shifted learned policies from 40\% deflection rates (baseline) to under 10\% while maintaining natural use of \textsc{AskQuestion} for genuine opinion elicitation and clarification.

\subsubsection{Transition Control: Content-Contingent Movement}

Early training showed agents attempting transitions prematurely—after only 1--2 facts per exhibit—which frustrated visitors and reduced coverage depth. Research on pacing in educational dialogue suggests transitions should occur only after sufficient content delivery \citep{koedinger2012knowledge,reimann2024dm}. We address this through a \textbf{transition insufficiency penalty} $r^{\text{trans}}_t$ that penalizes premature transitions:

\[
r^{\text{trans}}_t = 
\begin{cases}
-0.20 & \text{if } \textsc{OfferTransition} \text{ chosen at } t \text{ and } |\mathcal{F}_{\text{current}}| < 2 \\
-0.16 & \text{if } \textsc{OfferTransition} \text{ chosen at } t \text{ and } |\mathcal{F}_{\text{current}}| = 1 \\
0 & \text{otherwise (2+ facts or exemption)}
\end{cases}
\]

The penalty scales with insufficient coverage: transitioning with zero facts at the current exhibit incurs the full penalty ($-0.20$), while transitioning with exactly one fact incurs a reduced penalty ($-0.16$). Transitions with two or more facts are not penalized. Additionally, a \textit{3-turn exemption rule} applies: if a transition was successfully completed (visitor accepted) within the last 3 turns, the penalty is waived, allowing natural follow-up transitions after successful moves. This design encourages agents to provide substantive content before moving while maintaining flexibility for natural dialogue flow.\\

Empirically, this reduced premature transitions from 45\% of all transition attempts (baseline) to under 15\%, while maintaining successful transitions at 70--80\% acceptance rates. The exemption rule prevents over-penalization of natural multi-step navigation sequences.

\subsubsection{Conclude Bonus: Coverage Breadth Incentive}

To incentivize comprehensive coverage across multiple exhibits, we introduce a \textbf{conclude bonus} $r^{\text{conclude}}_t$ that rewards breadth of exploration when the agent chooses to end the session:

\[
r^{\text{conclude}}_t = 
\begin{cases}
0.2 \times |\{e \in \mathcal{E} : |\mathcal{F}_e^{\text{used}}| > 0\}| & \text{if } \textsc{Conclude} \text{ chosen at } t \\
0 & \text{otherwise}
\end{cases}
\]

The bonus scales linearly with the number of distinct exhibits that have at least one fact mentioned: $0.2$ per exhibit. This directly incentivizes the agent to explore diverse content rather than staying on a single exhibit, aligning with the museum's goal of providing comprehensive tours. The bonus is only awarded when the \textsc{Conclude} option is selected, creating a natural incentive to both explore broadly and recognize when sufficient coverage has been achieved.\\

This reward component complements the novelty reward: while novelty rewards \textit{within-exhibit} exploration (new facts per exhibit), the conclude bonus rewards \textit{across-exhibit} breadth (coverage diversity). Together, they encourage both depth and breadth of content delivery. Empirically, trained policies with the conclude bonus cover an average of 5--7 exhibits per session (vs. 2--3 without), while maintaining quality through the transition insufficiency penalty.\\

\subsubsection{Prompt Construction and Subaction Contracts}


\subsection{Action Execution and Grounding}
\label{sec:action-grounding}

The hierarchical policy selects high-level options and low-level subactions, but these must be realized as concrete utterances through the LLM-based dialogue planner. This section details how subactions are executed, how knowledge-graph grounding is enforced, and how hallucination is detected and penalized.

\subsubsection{Dialogue Action Tokens and Prompt Construction}
\label{sec:dat-templates}

The controller's job is long-range: commit to a conversational strategy and decide when to switch. Realization is short-range: say the next thing naturally, on style, and in line with the strategy. Prompt-engineered dialogue action tokens sit right on that boundary. Each option in a conversational strategy set (such as \textsc{Explain}, \textsc{AskQuestion}, \textsc{OfferTransition}, \textsc{Conclude}) could have a compact prompt header—a short, human-readable template with slots—that frames the next turn. The prompt would be paired with a context vector, and a few turn-aligned state bits from intent features. The LLM then realizes the turn freely under that header.\\

This is a pragmatic variant of plan-aware prompting that takes advantage of the fluency and flexibility of an LLM and long horizon decision making of HRL. Two strands of evidence support this. First, Conversation Routines show that reusable, option-like prompt schema with slots for context and constraints can keep generation ``on plan'' while preserving fluent surface language \citep{robino2025conversation}. Second, planning tokens show that small, prefixed step cues can improve the structure and reliability of reasoning, yielding stronger performance and analyses consistent with greater controllability; although these cues are learned rather than hand-templated, the control effect is analogous in spirit to our headers \citep{wang2024planning}. Taken together, these results motivate using lightweight, structured headers to shape the next turn while keeping the text natural.\\

The limitation is temporal: prompt headers decide what to say \emph{now}, not \emph{when} to change tactics. That decision sits with the strategic layer. As formalized earlier in the options/SMDP view \citep{sutton1999between}, a manager samples a strategy (option), an intra-option policy realizes utterances for several turns, and a termination function hands control back when the evidence says to switch. Under this split, the header is the realization-time contract for the active option; the termination function—not an arbitrary token budget—governs when the header should change.\\

Signals feeding the controller come from the earlier pieces. Turn-aligned intent cues summarize what the visitor is doing now and how confident that reading is; gaze dwell over the current object offers a robust, online proxy for attention. When intent shifts or dwell drops, termination should rise; when attention is steady and the intent calls for elaboration, the currently active option should persist and its header should continue to frame realization. This keeps multi-turn structure without sacrificing the flexibility that makes LLMs effective at the utterance level.

\subsubsection{Prompt Construction and Subaction Contracts}

Each subaction has a \textit{behavioral contract}: a short description of what the agent must do, which entities/facts it may reference, and which it must avoid. The dialogue planner implements this through subaction-specific prompt builders.

\paragraph{ExplainNewFact contract:} introduce a fact from the knowledge graph that has not yet been mentioned for the current exhibit. The prompt builder:

\begin{enumerate}
    \item Retrieves all facts for \texttt{current\_exhibit} from the knowledge graph.
    \item Filters out facts whose IDs are in \texttt{facts\_mentioned\_per\_exhibit[current\_exhibit]}.
    \item Constructs a prompt with header: \texttt{OPTION: Explain | SUBACTION: ExplainNewFact | FOCUS: [exhibit] | LAST\_UTT: [visitor's last utterance]}.
    \item Includes a context block listing the available new facts (with IDs) and the instruction: ``Choose ONE new fact and explain it in 2--3 sentences. You MUST include the fact ID in brackets, e.g., [EX\_001]. Directly respond to the visitor's last comment.''
\end{enumerate}

Example prompt excerpt:

\begin{quote}
\texttt{OPTION: Explain\\
SUBACTION: ExplainNewFact\\
FOCUS: The\_Annunciation\_Triptych\\
LAST\_UTT: ``What's the symbolism in this painting?''\\
\\
Available new facts:\\
- The dove represents the Holy Spirit [AN\_002]\\
- Gold leaf was used for divine figures [AN\_003]\\
\\
Your response (include fact ID):}
\end{quote}

The LLM then generates a response like: ``The dove you see at the top represents the Holy Spirit [AN\_002], a common symbol in Annunciation scenes. It signifies divine presence.''

\paragraph{OfferTransition contract:} assess current exhibit coverage, identify the exhibit with the most unexplained facts, and propose moving there. The prompt builder:

\begin{enumerate}
    \item Computes coverage for all exhibits: for each $e \in \mathcal{E}$, calculate $|\mathcal{F}_e^{\text{total}}| - |\mathcal{F}_e^{\text{used}}|$ (facts remaining).
    \item Selects $e^* = \arg\max_e (|\mathcal{F}_e^{\text{total}}| - |\mathcal{F}_e^{\text{used}}|)$ (exhibit with most content left).
    \item Constructs a prompt with header: \texttt{OPTION: OfferTransition | SUBACTION: SuggestMove | TARGET: [e*]}.
    \item Includes a context block showing current exhibit, facts discussed, and target exhibit with reason: ``We've covered [N] facts here. [Target exhibit] has [M] facts we haven't discussed yet.''
\end{enumerate}

Example prompt excerpt:

\begin{quote}
\texttt{OPTION: OfferTransition\\
SUBACTION: SuggestMove\\
CURRENT: The\_Annunciation\_Triptych (3 facts discussed)\\
TARGET: King\_Caspar (5 facts remaining)\\
\\
Suggest moving to King\_Caspar. Mention what we've covered here and why the next exhibit is interesting.}
\end{quote}

The LLM generates: ``We've explored the symbolism and materials of the Annunciation Triptych. Shall we move on to King Caspar? There are some fascinating details about his representation I'd love to share.''\\

This design ensures transitions are \textit{coverage-aware}: the agent doesn't just move randomly but selects the next exhibit based on unexplored content, aligning with the novelty reward and conclude bonus (which both depend on breadth of coverage). The prompt makes this reasoning explicit, both for interpretability and to guide the LLM's language (mentioning ``what we've covered'' reassures the visitor that their time was well-spent).

\subsubsection{Fact Verification and Hallucination Detection}

A critical failure mode for LLM-based agents is hallucination: generating plausible-sounding but false statements. In museum dialogue, this is unacceptable—accuracy is a core curatorial value \citep{serrell2015exhibit}. We enforce grounding through a post-generation verification step:

\begin{enumerate}
    \item \textbf{Extract fact IDs} from the agent's utterance via regex: \texttt{[A-Z]\{2\}\_\textbackslash d\{3\}}.
    \item \textbf{Validate against knowledge graph}: For each ID, check if it exists in \texttt{knowledge\_graph.get\_exhibit\_facts(current\_exhibit)}.
    \item \textbf{Classify facts} as: (i) \textit{new and valid} (in KB, not yet mentioned), (ii) \textit{repeat} (in KB, already mentioned), or (iii) \textit{hallucinated} (not in KB for this exhibit).
\end{enumerate}

Hallucinated facts trigger two consequences. First, they \textit{do not contribute} to the novelty reward: $r^{\text{nov}}_t$ only counts verified new facts. Second, they are logged and reported for debugging (though we do not directly penalize them in the reward, as early experiments showed this caused the agent to avoid providing any facts at all—an overcorrection). Instead, we rely on the absence of positive novelty reward and the fact that hallucinations often co-occur with off-topic or incoherent responses (which yield low dwell). Empirically, hallucination rates dropped from 8\% of all fact mentions early in training to under 2\% at convergence, suggesting the agent learned to stay on-topic and cite correctly without explicit penalties.\\

The implementation maintains per-exhibit fact sets:

\begin{verbatim}
self.facts_mentioned_per_exhibit: Dict[str, Set[str]] = {
    ex: set() for ex in self.exhibit_keys
}
\end{verbatim}

After each agent turn, the environment parses fact IDs, validates them, and updates the set for the current exhibit. This simple mechanism provides strong grounding guarantees: only validated facts count toward rewards and only facts from the current exhibit's knowledge subgraph are valid, preventing cross-contamination (e.g., mentioning a fact about King Caspar while discussing the Annunciation Triptych).

\subsection{Combined Reward Function}
\label{sec:combined-reward}

The complete reward function combines all components into a single per-turn signal:

\[
R_t = r^{\text{eng}}_t + r^{\text{nov}}_t + r^{\text{resp}}_t + r^{\text{trans}}_t + r^{\text{conclude}}_t
\]

where:

\begin{itemize}
    \item $r^{\text{eng}}_t = \text{dwell}_t$: Engagement reward equals the visitor's gaze dwell time on the current exhibit. This is reduced by simulator-level penalties for transition spam (repeated transition offers without visitor acceptance).
    
    \item $r^{\text{nov}}_t = 0.15 \times |\mathcal{F}^{\text{new}}_t|$: Novelty reward scales with the number of new, validated facts mentioned at turn $t$ (scale factor $\alpha = 0.15$).
    
    \item $r^{\text{resp}}_t \in \{+0.25, -0.15, 0\}$: Responsiveness reward incentivizes answering visitor questions ($+0.25$) and penalizes deflection with counter-questions ($-0.15$).

    \item $r^{\text{conclude}}_t = 0.2 \times |\{e \in \mathcal{E} : |\mathcal{F}_e^{\text{used}}| > 0\}|$ if \textsc{Conclude} chosen, else $0$: Conclude bonus rewards coverage breadth (0.2 per exhibit covered).
\end{itemize}

This reward structure balances immediate engagement signals (dwell time), content quality (novelty, responsiveness), and strategic behavior (transition control, coverage breadth). The components operate at different timescales: engagement and novelty are turn-level, responsiveness is turn-pair dependent, transition control is option-level, and the conclude bonus is session-level. Together, they guide the agent toward informative, engaging, and well-paced dialogue that explores diverse museum content.

\subsection{Action Masking and Strategic Constraints}
\label{sec:action-masking-detail}

To prevent structurally invalid actions and guide the policy toward coherent strategies, we implement action masking at both the option and subaction levels. Masking dynamically adjusts the available action set based on the current dialogue state, effectively encoding domain constraints as hard constraints on the policy \citep{huang2020action,tang2021discovering}.

\subsubsection{Option-Level Masking}

The primary option-level constraint is on \textsc{Conclude}: the agent may not end the session prematurely. We define two thresholds:

\begin{itemize}
    \item \texttt{min\_facts\_before\_conclude = 3}: At least 3 facts must have been mentioned across all exhibits.
    \item \texttt{min\_exhibits\_before\_conclude = 2}: At least 2 distinct exhibits must have been discussed (i.e., have $|\mathcal{F}_e| > 0$).
\end{itemize}

If either condition is unmet, \textsc{Conclude} is masked (unavailable). This prevents the pathological policy of concluding immediately for safety, which occasionally emerged in early unconstrained training (yielding zero novelty and conclude bonus, but avoiding any risk of low dwell from bad explanations—a local minimum).\\

The thresholds are set based on minimal tour expectations: a ``guided tour'' that covers only one exhibit with 1--2 facts is not a tour at all. The values (3 facts, 2 exhibits) represent the bare minimum for a coherent session. Empirically, trained policies typically exceed these thresholds significantly (avg. 8--12 facts, 5--7 exhibits), so the masking acts as a safety guardrail rather than a binding constraint during normal operation.

\subsubsection{Subaction-Level Masking}

Within the \textsc{Explain} option, we mask subactions based on knowledge availability:

\begin{itemize}
    \item \textbf{ExplainNewFact} is masked if all facts for the current exhibit have already been mentioned ($|\mathcal{F}_{\text{current}}^{\text{new}}| = 0$). This prevents the agent from attempting to introduce ``new'' facts when none remain, which would force the LLM to either hallucinate or produce a generic non-explanation.
    \item \textbf{RepeatFact} is masked if no facts have been mentioned yet for the current exhibit ($|\mathcal{F}_{\text{current}}^{\text{used}}| = 0$). This prevents the agent from trying to ``repeat'' something that was never said.
    \item \textbf{ClarifyFact} is always available (no masking), as clarification can be provided even without explicit fact repetition (e.g., defining terms, providing analogies based on prior context).
\end{itemize}

These masks are implemented via \texttt{\_get\_available\_subactions(option)}, which returns a filtered list of subactions. The actor's subaction selection is then clamped to this list: if the actor's raw output would select an invalid subaction, it is replaced with the first available one. This ensures the prompt builder always receives a valid subaction, maintaining the subaction contract.\\

Masking serves two purposes. First, it provides \textit{safety}: the agent cannot produce structurally nonsensical moves (e.g., repeating a fact that was never mentioned). Second, it provides \textit{pedagogical guidance}: by disabling \textit{ExplainNewFact} when the exhibit is exhausted, we force the agent to either ask questions, transition, or conclude—preventing it from ``spinning'' on the same exhibit indefinitely. Studies of hierarchical RL show that task-structure-aware masking improves both sample efficiency and final performance by pruning the action space to semantically valid choices \citep{huang2020action,tang2021discovering}.


\section{Hypotheses}

The system couples an option-based controller with actor–critic learning and compact prompt headers. The hypotheses below mirror that design: strategy and timing at the top; wording and grounding at the surface.

\paragraph{Hypothesis 1}

An option-based manager will outperform a flat policy on long-horizon objectives. Concretely, we expect higher episodic return with lower variance, longer coherent stretches under a chosen strategy, and fewer needless switches between strategies when both are trained under the same rewards and prompts.

\paragraph{Hypothesis 2}

Learned terminations for \textsc{Explain} will track engagement and intent: when dwell is high and the visitor’s intent supports explanation, \textsc{Explain} should persist; when dwell falls or intent shifts, it should end sooner. We expect a positive correlation between dwell and \textsc{Explain} duration, and earlier terminations following detected intent changes.


\paragraph{Hypothesis 3}

Slot-filled prompt headers tied to the active option will improve the next turn’s realization: higher faithfulness to the intended move, tighter grounding to the exhibit KB, and less repetition. Concretely, we expect higher novel-fact coverage, a lower repetition ratio, a higher KB-citation/grounding rate with fewer off-KB claims, and better on-plan consistency with the chosen option/subaction.

\paragraph{Hypothesis 4}

With semi-Markov returns, actor–critic will train the hierarchy smoothly. Relative to a flat actor–critic (no options) and a fixed-duration hierarchy (one-turn options), we expect steadier learning curves (fewer spikes), smaller update magnitudes, and faster time-to-target reward, while preserving sensible option lengths.

\paragraph{Hypothesis 5}

Replacing DialogueBERT’s full 149-d state (Section~\ref{sec:state-representation}) with a low-dimensional dialogue-act trace (one-hot over recent act types plus option usage) will reduce performance. If lexical semantics are essential, the ablated state should show lower return, poorer coverage, and weaker responsiveness relative to the full embedding setup.

\paragraph{Hypothesis 6}

Removing the transition-acceptance shaping term from the reward (Section~\ref{sec:reward-design})—i.e., training without the probabilistic success bonus/penalty—will degrade pacing. Without this signal, we expect earlier, less-informed transitions, lower dwell prior to transitions, and reduced exhibit coverage compared to the shaped variant.

\paragraph{Hypothesis 7}

Using DialogueBERT's turn-aware and role-aware embeddings for multi-turn dialogue context ($\mathbf{c}_t$) while keeping standard BERT for single-utterance intent ($\mathbf{i}_t$) will improve dialogue coherence and context-dependent responses. Since $\mathbf{c}_t$ encodes the last 3 exchanges, DialogueBERT's hierarchical structure should better capture turn relationships and speaker roles, leading to more contextually appropriate responses. We expect higher dialogue coherence scores, better handling of multi-turn references, improved responsiveness to context-dependent questions, and reduced contradictions compared to using standard BERT for both components.

\section{Evaluation/Experimental Design}

We evaluate entirely in the simulator so comparisons are clean and repeatable. Each run fixes the exhibit itinerary, persona seed, and knowledge-base snapshot; only the model variant changes. For every condition we generate \(N\) tours (e.g., \(N{=}\)T.B.Discussed) across multiple random seeds and report means, standard errors, and paired statistics wherever possible (same seed, different model). Unless noted, all models share the same prompts, rewards, and decoding settings.\\


\textbf{H1: option structure and long-horizon behavior.}
For each tour we log episodic return and the strategy timeline. We report mean return with variance, average coherent-span length (consecutive turns under the same option), and switch rate (switches per 100 turns). Improvements over the flat baseline support H1. We use paired \(t\)-tests or Wilcoxon signed-rank tests (per-seed pairing) and report effect sizes.\\

\textbf{H2: learned pacing for \textsc{Explain}.}
We align \textsc{Explain} segments with dwell and intent. Two checks: (i) correlation between segment length \(\tau\) and mean dwell within the segment (expect \(\rho{>}0\)); (ii) time-to-termination after a detected intent change away from “explain,” compared to matched no-shift periods. We plot duration vs.\ dwell with confidence bands, report correlations with CIs, and use simple pre/post windows around intent changes to quantify earlier endings.\\

\textbf{H3: headers and local realization}
We test whether headers do what they claim by checking outcomes against their constraints. Metrics: novel-fact coverage \(|\mathcal{F}_{\text{used}}|/|\mathcal{F}_{\text{total}}|\); repetition ratio (re-mentions beyond first / total fact mentions); grounding precision/recall via a KB aligner; hallucination rate (claims with no KB match); and on-plan compliance (e.g., \textit{ExplainNewFact} introduces a new fact ID; \textit{Clarify/Repeat} avoid new IDs; \textit{Ask*} ends with a question). A lightweight act classifier (text-only) provides an independent check that realized acts match the header; high agreement indicates reliable steering. As a predictive check, we correlate “header completeness” (share of slots filled: \texttt{FOCUS}, \texttt{LAST\_UTT}, fact IDs) with violations and hallucinations; negative correlations support the claim that richer headers reduce errors.\\

\textbf{H4: training stability with actor–critic.}
We track learning curves (smoothed return vs.\ updates), update magnitudes (\(\ell_2\)-norm of parameter steps, KL per update), time-to-target (episodes to reach a fixed return), and option duration sanity (median \(\tau\) per option). The hierarchical actor–critic should show smoother curves (fewer large spikes), smaller average update norms, and faster time-to-target than the flat actor–critic, while keeping sensible option lengths.\\

\textbf{H5: semantic state ablation.}
We train a variant whose observation replaces DialogueBERT embeddings with a compact dialogue-act summary (one-hot of the last 3 utterance act labels plus option usage counts). Metrics: episodic return, coverage (\(|\mathcal{F}_{\text{used}}|/|\mathcal{F}_{\text{total}}|\)), dwell averages, and act-consistency (agreement between predicted option/subaction and realized utterance classified by the act classifier). We also report responsiveness rate (fraction of visitor questions answered with new facts). Paired seeds against the full state allow Wilcoxon tests; large drops support H5.\\

\textbf{H6: transition shaping ablation.}
We compare the full reward to a version without the transition sufficiency bonus and insufficiency penalty. Metrics centre on pacing: transition success rate, mean dwell in the turn preceding a transition, number of facts shared per exhibit before leaving, and total coverage/return. We also track confusion responses triggered after transitions as a proxy for premature moves. Paired seeds and paired \(t\)-tests quantify the effect of removing the shaping term.\\

\textbf{H7: hybrid DialogueBERT for multi-turn context.}
We compare two variants: (i) baseline with standard BERT for both $\mathbf{i}_t$ and $\mathbf{c}_t$; (ii) hybrid with DialogueBERT (turn/role embeddings) for $\mathbf{c}_t$ and standard BERT for $\mathbf{i}_t$. Metrics focus on multi-turn understanding: dialogue coherence (measured via reference resolution accuracy—can the agent correctly interpret pronouns and ellipsis that refer to earlier turns?); context-dependent question answering (fraction of questions that require multi-turn context answered correctly); turn-aware response appropriateness (human evaluation or automated classifier rating responses as contextually appropriate vs.\ generic); and contradiction rate (instances where agent contradicts earlier statements). We also track embedding similarity between consecutive turns to measure context continuity. Paired seeds and paired $t$-tests quantify the benefit of DialogueBERT's turn/role embeddings for multi-turn context encoding.\\


\section{Results}


\section{Discussion}

\bibliographystyle{apalike}
\bibliography{References}
\end{document}















