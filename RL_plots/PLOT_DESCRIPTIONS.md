# RL/HRL Plot Descriptions

This document describes each plot generated by the RL plot generator, their purpose, and how to interpret them.

## 1. Learning Curves

### 01_learning_curve.png
**Purpose:** Standard RL metric showing agent performance improvement over training.

**What to look for:**
- Upward trend indicates learning
- Convergence when curve plateaus
- High variance may indicate unstable training
- Confidence intervals show reliability

**Use in paper:** Primary evaluation metric for RL performance.

### 02_episode_length.png
**Purpose:** Sample efficiency - how episode duration changes over training.

**What to look for:**
- Longer episodes may indicate better exploration
- Decreasing length may indicate premature termination
- Stability suggests consistent policy behavior

**Use in paper:** Evidence of sample efficiency and exploration quality.

## 2. Value Function Analysis

### 03_value_function.png
**Purpose:** Critic network evolution - how value estimates change.

**What to look for:**
- Increasing values indicate better state evaluation
- Convergence suggests stable value function
- Correlation with returns validates critic quality

**Use in paper:** Demonstrates value function learning and critic quality.

## 3. Policy Gradient Metrics

### 04_policy_loss.png
**Purpose:** Actor (policy) loss evolution.

**What to look for:**
- Decreasing loss indicates improving policy
- Oscillations may indicate learning rate issues
- Convergence suggests stable policy

**Use in paper:** Shows policy gradient learning progress.

### 05_value_loss.png
**Purpose:** Critic (value) loss evolution.

**What to look for:**
- Decreasing loss indicates better value estimation
- Should correlate with value function plot
- Low loss = accurate value predictions

**Use in paper:** Demonstrates critic learning quality.

### 06_policy_entropy.png
**Purpose:** Exploration vs exploitation trade-off.

**What to look for:**
- High entropy = more exploration
- Low entropy = more exploitation
- Gradual decrease indicates natural explorationâ†’exploitation transition

**Use in paper:** Evidence of balanced exploration-exploitation.

## 4. Advantage Analysis

### 07_advantage_distribution.png
**Purpose:** Distribution of advantage values (policy gradient signal quality).

**What to look for:**
- Centered around zero is ideal
- Positive mean = optimistic estimates
- Negative mean = pessimistic estimates
- Shape indicates signal quality

**Use in paper:** Validates advantage estimation quality for policy gradients.

## 5. HRL-Specific Analysis

### 08_option_transitions.png
**Purpose:** Option transition patterns (HRL structure).

**What to look for:**
- Diagonal elements = option persistence
- Off-diagonal = transitions between options
- Probabilities show hierarchical policy structure
- Reveals strategy patterns

**Use in paper:** Demonstrates hierarchical control and option usage patterns (RQ1).

### 09_option_durations.png
**Purpose:** Temporal abstraction quality - how long options persist.

**What to look for:**
- Longer durations = better temporal abstraction
- Mean values indicate option persistence
- Variance shows consistency
- Different durations per option = diverse strategies

**Use in paper:** Evidence of temporal abstraction and multi-turn coherence (RQ1).

## 6. Reward Analysis

### 10_reward_distribution.png
**Purpose:** Reward variability and distribution shape.

**What to look for:**
- Mean vs median shows skewness
- Variance indicates performance consistency
- Distribution shape reveals reward structure

**Use in paper:** Shows reward signal quality and performance variability.

### 11_reward_decomposition.png
**Purpose:** Contribution of each reward component over time.

**What to look for:**
- Component trends show what drives learning
- Balance between components
- Evolution indicates policy adaptation

**Use in paper:** Demonstrates reward shaping effectiveness.

## 7. Training Stability

### 12_training_stability.png
**Purpose:** Training convergence and stability metrics.

**What to look for:**
- Decreasing variance = convergence
- Coefficient of variation = stability metric
- Lower CV = more stable training

**Use in paper:** Evidence of stable, convergent training.

## 8. Policy Analysis

### 13_action_distribution.png
**Purpose:** Action (option) usage frequency.

**What to look for:**
- Balanced distribution = diverse strategy use
- Skewed distribution = preference for certain options
- Evolution shows policy adaptation

**Use in paper:** Demonstrates option diversity and policy behavior.

## Interpretation Guidelines

### For RL Evaluation:
- Focus on learning curves (01, 02)
- Check policy/value losses (04, 05)
- Verify entropy behavior (06)
- Examine advantage distribution (07)

### For HRL Evaluation:
- Analyze option transitions (08)
- Check option durations (09)
- Verify hierarchical structure

### For Training Quality:
- Check stability plots (12)
- Examine reward distribution (10)
- Analyze component decomposition (11)

## Integration with Existing Plots

These RL plots complement the domain-specific plots in `create_evaluation_plots.py`:
- **RL plots** focus on RL/HRL algorithms (value functions, policy gradients, advantages)
- **Domain plots** focus on application metrics (coverage, engagement, content quality)

Together they provide comprehensive evaluation of both the learning algorithm and the application domain.
